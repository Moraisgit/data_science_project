{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Declare path to project directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_directory = \"/home/morais/data_science_project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variable Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame\n",
    "from dslabs_functions import get_variable_types, encode_cyclic_variables, dummify\n",
    "\n",
    "data: DataFrame = read_csv(f\"{path_to_directory}/datasets/class_ny_arrests.csv\", index_col=\"ARREST_KEY\", na_values=\"\")\n",
    "\n",
    "jurisdiction_code: dict[str, int] = {\"NY\": 0, \"nonNY\": 1}\n",
    "arrest_boro: dict[str, int] = {\"Q\": 1, \"M\": 2, \"K\": 3, \"B\": 4, \"S\": 5}\n",
    "perp_races: dict[str, int] = {\"UNKNOWN\": 0, \"BLACK\": 1, \"WHITE HISPANIC\": 2, \"BLACK HISPANIC\": 3,\n",
    "                              \"WHITE\": 4, \"ASIAN / PACIFIC ISLANDER\": 5, \"AMERICAN INDIAN/ALASKAN NATIVE\": 6, \"OTHER\": 7}\n",
    "law_cat_cd: dict[str, int] = {\"M\": 0, \"F\": 1}\n",
    "\n",
    "data['JURISDICTION_CODE'] = data['JURISDICTION_CODE'].apply(lambda x: 'NY' if x < 3 else 'nonNY')\n",
    "\n",
    "encoding: dict[str, dict[str, int]] = {\n",
    "    \"ARREST_BORO\": arrest_boro,\n",
    "    \"PERP_RACE\": perp_races,\n",
    "    \"LAW_CAT_CD\": law_cat_cd,\n",
    "    \"PERP_SEX\": law_cat_cd,\n",
    "    \"JURISDICTION_CODE\": jurisdiction_code,\n",
    "}\n",
    "data: DataFrame = data.replace(encoding, inplace=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_group_mapping = {\n",
    "    '<18': 1,\n",
    "    '18-24': 2,\n",
    "    '25-44': 3,\n",
    "    '45-64': 4,\n",
    "    '65+': 5,\n",
    "    'UNKNOWN': 0\n",
    "}\n",
    "\n",
    "data[\"AGE_GROUP\"] = data[\"AGE_GROUP\"].apply(\n",
    "    lambda x: age_group_mapping.get(x, 0)  # Map valid groups; default to 0 for invalid\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import pi, sin, cos\n",
    "\n",
    "# Função para codificar variáveis cíclicas\n",
    "def encode_cyclic_variables(data: DataFrame, vars: list[str]) -> None:\n",
    "    for v in vars:\n",
    "        x_max = max(data[v])  # Valor máximo da variável\n",
    "        data[v + \"_sin\"] = data[v].apply(lambda x: round(sin(2 * pi * x / x_max), 3))\n",
    "        data[v + \"_cos\"] = data[v].apply(lambda x: round(cos(2 * pi * x / x_max), 3))\n",
    "    return data\n",
    "\n",
    "# Transformar a coluna \"ARREST_DATE\"\n",
    "data[\"ARREST_DATE\"] = pd.to_datetime(data[\"ARREST_DATE\"], format=\"%m/%d/%Y\")\n",
    "data[\"Year\"] = data[\"ARREST_DATE\"].dt.year\n",
    "data[\"Month\"] = data[\"ARREST_DATE\"].dt.month\n",
    "data[\"Day\"] = data[\"ARREST_DATE\"].dt.day\n",
    "\n",
    "# Criar variáveis cíclicas também para mês (opcional)\n",
    "data = encode_cyclic_variables(data, [\"Month\"])\n",
    "\n",
    "# Remover colunas originais (opcional)\n",
    "data = data.drop(columns=[\"ARREST_DATE\", \"Month\"])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover colunas\n",
    "data = data.drop(columns=[\"PD_DESC\", \"LAW_CODE\"])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Criando um LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Transformando os valores em inteiros\n",
    "data['OFNS_DESC'] = label_encoder.fit_transform(data['OFNS_DESC'])\n",
    "\n",
    "# Criando um LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Transformando os valores em inteiros\n",
    "data['PD_CD'] = label_encoder.fit_transform(data['PD_CD'])\n",
    "\n",
    "# Criando um LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Transformando os valores em inteiros\n",
    "data['KY_CD'] = label_encoder.fit_transform(data['KY_CD'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: DataFrame = data.copy(deep=True)\n",
    "df.to_csv(f\"{path_to_directory}/datasets/class_ny_arrests_encoding.csv\", index=False)\n",
    "print(f\"Data after encoding: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Methodology After Variable Encoding</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from pandas import DataFrame, read_csv\n",
    "from matplotlib.pyplot import savefig, show, figure\n",
    "from dslabs_functions import plot_multibar_chart, CLASS_EVAL_METRICS, run_KNN, run_NB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dslabs_functions import DELTA_IMPROVE\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "\n",
    "def run_NB(trnX, trnY, tstX, tstY, metric: str = \"accuracy\") -> dict[str, float]:\n",
    "    estimators: dict[str, GaussianNB | BernoulliNB] = {\n",
    "        \"GaussianNB\": GaussianNB(),\n",
    "        \"BernoulliNB\": BernoulliNB(),\n",
    "    }\n",
    "    best_model: GaussianNB | BernoulliNB = None\n",
    "    best_performance: float = 0.0\n",
    "    eval: dict[str, float] = {}\n",
    "\n",
    "    for clf in estimators:\n",
    "        estimators[clf].fit(trnX, trnY)\n",
    "        prdY: ndarray = estimators[clf].predict(tstX)\n",
    "        performance: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "        if performance - best_performance > DELTA_IMPROVE:\n",
    "            best_performance = performance\n",
    "            best_model = estimators[clf]\n",
    "    if best_model is not None:\n",
    "        prd: ndarray = best_model.predict(tstX)\n",
    "        for key in CLASS_EVAL_METRICS:\n",
    "            eval[key] = CLASS_EVAL_METRICS[key](tstY, prd)\n",
    "    return eval\n",
    "\n",
    "def evaluate_approach(\n",
    "    train: DataFrame, test: DataFrame, target: str = \"class\", metric: str = \"accuracy\"\n",
    ") -> dict[str, list]:\n",
    "    trnY = train.pop(target).values\n",
    "    trnX: ndarray = train.values\n",
    "    tstY = test.pop(target).values\n",
    "    tstX: ndarray = test.values\n",
    "    eval: dict[str, list] = {}\n",
    "\n",
    "    eval_NB: dict[str, float] = run_NB(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    eval_KNN: dict[str, float] = run_KNN(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    if eval_NB != {} and eval_KNN != {}:\n",
    "        for met in CLASS_EVAL_METRICS:\n",
    "            eval[met] = [eval_NB[met], eval_KNN[met]]\n",
    "    return eval\n",
    "\n",
    "target = \"JURISDICTION_CODE\"\n",
    "file_tag = \"class_ny_arrests\"\n",
    "filename = \"datasets/class_ny_arrests.csv\"\n",
    "\n",
    "new_data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_encoding.csv\")\n",
    "new_data = new_data.dropna()\n",
    "\n",
    "new_data = new_data.head(100000)\n",
    "\n",
    "train, test = train_test_split(new_data, test_size=0.2, random_state=42)\n",
    "\n",
    "figure()\n",
    "eval_basic: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "print(eval_basic)\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval_basic, title=f\"Encoding evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - Encoding results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Variable Encoding**\n",
    "## **Drop Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar missing values e \"Unknowns\" na idade\n",
    "data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_encoding.csv\", na_values=\"\", index_col=None, sep=',', decimal='.')\n",
    "data = data.dropna()\n",
    "\n",
    "data = data[data['AGE_GROUP'] != 0]\n",
    "\n",
    "data = data[data['OFNS_DESC'] != 72]\n",
    "\n",
    "data = data[data['PERP_RACE'] != 0]\n",
    "\n",
    "data = data[data['PD_CD'] != 279]\n",
    "\n",
    "data = data[data['KY_CD'] != 60]\n",
    "\n",
    "df: DataFrame = data.copy(deep=True)\n",
    "df.to_csv(f\"{path_to_directory}/datasets/{file_tag}_missing_values.csv\", index=False)\n",
    "print(f\"Data after missing Values: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodology of Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_missing_values.csv\")\n",
    "new_data = new_data.head(200000)\n",
    "\n",
    "train, test = train_test_split(new_data, test_size=0.2, random_state=42)\n",
    "\n",
    "figure()\n",
    "eval_missing: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "print(eval_missing)\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval_missing, title=f\"Drop missing values evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - Drop missing values results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Filling missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar missing values e \"Unknowns\" na idade\n",
    "data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_encoding.csv\", na_values=\"\", index_col=None, sep=',', decimal='.')\n",
    "\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "df: DataFrame = data.copy(deep=True)\n",
    "df.to_csv(f\"{path_to_directory}/datasets/{file_tag}_filling_values.csv\", index=False)\n",
    "print(f\"Data after missing Values: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodology of Filling Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_filling_values.csv\")\n",
    "new_data = new_data.head(200000)\n",
    "\n",
    "train, test = train_test_split(new_data, test_size=0.2, random_state=42)\n",
    "\n",
    "figure()\n",
    "eval_filling: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "print(eval_filling)\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval_filling, title=f\"Fill missing values evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - Fill missing values results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier method evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = {\"original\": eval_basic, \"missing\": eval_missing, \"filling\": eval_filling}\n",
    "\n",
    "best_mean = -float(\"inf\")\n",
    "best_mean_NB = -float(\"inf\")\n",
    "best_mean_KNN = -float(\"inf\")\n",
    "best_eval = None\n",
    "best_eval_NB = None\n",
    "best_eval_KNN = None\n",
    "best_eval_dict = None\n",
    "\n",
    "for eval_name, eval_dict in evaluations.items():\n",
    "    # Calculate mean for NB and KNN using generator expressions\n",
    "    mean_NB = sum(eval_dict[metric][0] for metric in [\"accuracy\", \"recall\", \"precision\", \"auc\", \"f1\"]) / 5\n",
    "    mean_KNN = sum(eval_dict[metric][1] for metric in [\"accuracy\", \"recall\", \"precision\", \"auc\", \"f1\"]) / 5\n",
    "    \n",
    "    # Print means\n",
    "    print(f\"{eval_name}: Mean NB = {mean_NB}, Mean KNN = {mean_KNN}\")\n",
    "\n",
    "    # Update the best mean for NB\n",
    "    if mean_NB > best_mean_NB:\n",
    "        best_mean_NB = mean_NB\n",
    "        best_eval_NB = eval_name\n",
    "\n",
    "    # Update the best mean for KNN\n",
    "    if mean_KNN > best_mean_KNN:\n",
    "        best_mean_KNN = mean_KNN\n",
    "        best_eval_KNN = eval_name\n",
    "\n",
    "# Print the best mean scores\n",
    "print(f\"\\nBest Mean NB: {best_mean_NB} from {best_eval_NB}\")\n",
    "print(f\"Best Mean KNN: {best_mean_KNN} from {best_eval_KNN}\")\n",
    "\n",
    "if best_mean_NB > best_mean_KNN:\n",
    "    best_mean = best_mean_NB\n",
    "    best_eval = best_eval_NB\n",
    "else:\n",
    "    best_mean = best_mean_KNN\n",
    "    best_eval = best_eval_KNN\n",
    "\n",
    "print(f\"\\nContinuing with {best_eval} with mean {best_mean}\")\n",
    "\n",
    "# Determine the filename based on the best evaluations\n",
    "if best_eval == \"original\":\n",
    "    filename = f\"{path_to_directory}/datasets/{file_tag}_encoding.csv\"\n",
    "    best_eval_dict = eval_basic\n",
    "elif best_eval == \"missing\":\n",
    "    filename = f\"{path_to_directory}/datasets/{file_tag}_missing_values.csv\"\n",
    "    best_eval_dict = eval_missing\n",
    "elif best_eval == \"filling\":\n",
    "    filename = f\"{path_to_directory}/datasets/{file_tag}_filling_values.csv\"\n",
    "    best_eval_dict = eval_filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Droping the missing values has a litle better values, so we will use that one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outliers Inputation**\n",
    "<h3>Drop Outliers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, Series\n",
    "from dslabs_functions import (\n",
    "    NR_STDEV,\n",
    "    get_variable_types,\n",
    "    determine_outlier_thresholds_for_var,\n",
    ")\n",
    "\n",
    "data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_missing_values.csv\")\n",
    "\n",
    "print(f\"Original train data: {data.shape}\")\n",
    "\n",
    "n_std: int = NR_STDEV\n",
    "n_std = 9\n",
    "print(\"n_std=\", n_std)\n",
    "\n",
    "numeric_vars: list[str] = get_variable_types(data)[\"numeric\"]\n",
    "\n",
    "if numeric_vars is not None:\n",
    "    df: DataFrame = data.copy(deep=True)\n",
    "    summary5: DataFrame = data[numeric_vars].describe()\n",
    "    for var in numeric_vars:\n",
    "        top_threshold, bottom_threshold = determine_outlier_thresholds_for_var(\n",
    "            summary5[var], threshold= n_std\n",
    "        )\n",
    "        outliers: Series = df[(df[var] > top_threshold) | (df[var] < bottom_threshold)]\n",
    "        df.drop(outliers.index, axis=0, inplace=True)\n",
    "    df.to_csv(f\"{path_to_directory}/datasets/{file_tag}_drop_outliers.csv\", index=False)\n",
    "    print(f\"Data after dropping outliers: {df.shape}\")\n",
    "else:\n",
    "    print(\"There are no numeric variables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodology of Droping Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_drop_outliers.csv\", na_values=\"\", index_col=None, sep=',', decimal='.')\n",
    "new_data = new_data.head(200000)\n",
    "\n",
    "train, test = train_test_split(new_data, test_size=0.2, random_state=42)\n",
    "\n",
    "figure()\n",
    "eval_dropping: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "print(eval_dropping)\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval_dropping, title=f\"Drop outliers evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - Outliers first approach (drop) results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Replacing outliers with fixed value</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [] != numeric_vars:\n",
    "    df: DataFrame = data.copy(deep=True)\n",
    "    for var in numeric_vars:\n",
    "        top, bottom = determine_outlier_thresholds_for_var(summary5[var])\n",
    "        median: float = df[var].median()\n",
    "        df[var] = df[var].apply(lambda x: median if x > top or x < bottom else x)\n",
    "    df.to_csv(f\"{path_to_directory}/datasets/{file_tag}_replacing_outliers.csv\", index=False)\n",
    "    print(\"Data after replacing outliers:\", df.shape)\n",
    "    print(df.describe())\n",
    "else:\n",
    "    print(\"There are no numeric variables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodology of Replacing Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_replacing_outliers.csv\", na_values=\"\", index_col=None, sep=',', decimal='.')\n",
    "new_data = new_data.head(200000)\n",
    "\n",
    "train, test = train_test_split(new_data, test_size=0.2, random_state=42)\n",
    "\n",
    "figure()\n",
    "eval_replacing: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "print(eval_replacing)\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval_replacing, title=f\"Replace outliers evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - Outliers second approach (replacing) results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Truncate outliers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [] != numeric_vars:\n",
    "    df: DataFrame = data.copy(deep=True)\n",
    "    for var in numeric_vars:\n",
    "        top, bottom = determine_outlier_thresholds_for_var(summary5[var])\n",
    "        df[var] = df[var].apply(\n",
    "            lambda x: top if x > top else bottom if x < bottom else x\n",
    "        )\n",
    "    df.to_csv(f\"{path_to_directory}/datasets/{file_tag}_truncate_outliers.csv\", index=False)\n",
    "    print(\"Data after truncating outliers:\", df.shape)\n",
    "    print(df.describe())\n",
    "else:\n",
    "    print(\"There are no numeric variables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodology of Truncating Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_truncate_outliers.csv\", na_values=\"\", index_col=None, sep=',', decimal='.')\n",
    "new_data = new_data.head(200000)\n",
    "\n",
    "train, test = train_test_split(new_data, test_size=0.2, random_state=42)\n",
    "\n",
    "figure()\n",
    "eval_truncate: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "print(eval_truncate)\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval_truncate, title=f\"Truncate outliers evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - Outliers third approach (truncate) results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier method evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = {\"original\": eval_missing, \"drop\": eval_dropping, \"replacing\": eval_replacing, \"truncate\": eval_truncate}\n",
    "\n",
    "best_mean = -float(\"inf\")\n",
    "best_mean_NB = -float(\"inf\")\n",
    "best_mean_KNN = -float(\"inf\")\n",
    "best_eval = None\n",
    "best_eval_NB = None\n",
    "best_eval_KNN = None\n",
    "best_eval_dict = None\n",
    "\n",
    "for eval_name, eval_dict in evaluations.items():\n",
    "    # Calculate mean for NB and KNN using generator expressions\n",
    "    mean_NB = sum(eval_dict[metric][0] for metric in [\"accuracy\", \"recall\", \"precision\", \"auc\", \"f1\"]) / 5\n",
    "    mean_KNN = sum(eval_dict[metric][1] for metric in [\"accuracy\", \"recall\", \"precision\", \"auc\", \"f1\"]) / 5\n",
    "    \n",
    "    # Print means\n",
    "    print(f\"{eval_name}: Mean NB = {mean_NB}, Mean KNN = {mean_KNN}\")\n",
    "\n",
    "    # Update the best mean for NB\n",
    "    if mean_NB > best_mean_NB:\n",
    "        best_mean_NB = mean_NB\n",
    "        best_eval_NB = eval_name\n",
    "\n",
    "    # Update the best mean for KNN\n",
    "    if mean_KNN > best_mean_KNN:\n",
    "        best_mean_KNN = mean_KNN\n",
    "        best_eval_KNN = eval_name\n",
    "\n",
    "# Print the best mean scores\n",
    "print(f\"\\nBest Mean NB: {best_mean_NB} from {best_eval_NB}\")\n",
    "print(f\"Best Mean KNN: {best_mean_KNN} from {best_eval_KNN}\")\n",
    "\n",
    "if best_mean_NB > best_mean_KNN:\n",
    "    best_mean = best_mean_NB\n",
    "    best_eval = best_eval_NB\n",
    "else:\n",
    "    best_mean = best_mean_KNN\n",
    "    best_eval = best_eval_KNN\n",
    "\n",
    "print(f\"\\nContinuing with {best_eval} with mean {best_mean}\")\n",
    "\n",
    "# Determine the filename based on the best evaluations\n",
    "if best_eval == \"original\":\n",
    "    filename = f\"{path_to_directory}/datasets/{file_tag}_missing_values.csv\"\n",
    "    best_eval_dict = eval_missing\n",
    "elif best_eval == \"drop\":\n",
    "    filename = f\"{path_to_directory}/datasets/{file_tag}_drop_outliers.csv\"\n",
    "    best_eval_dict = eval_dropping\n",
    "elif best_eval == \"replacing\":\n",
    "    filename = f\"{path_to_directory}/datasets/{file_tag}_replacing_outliers.csv\"\n",
    "    best_eval_dict = eval_replacing\n",
    "elif best_eval == \"truncate\":\n",
    "    filename = f\"{path_to_directory}/datasets/{file_tag}_truncate_outliers.csv\"\n",
    "    best_eval_dict = eval_truncate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing every diferent case of dealing with the outliers we see that the values don´t improve in any of the cases, so we will keep the missing values dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scaling**\n",
    "<h3>Methodollogy MinMax Scaler</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, Series\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_missing_values.csv\")\n",
    "\n",
    "target = \"JURISDICTION_CODE\"\n",
    "\n",
    "# Separate the target column\n",
    "target_data: Series = data.pop(target).copy()  # Use .copy() to ensure immutability\n",
    "\n",
    "# Scale only the feature variables\n",
    "scaler = StandardScaler(with_mean=True, with_std=True, copy=True)\n",
    "scaled_features = scaler.fit_transform(data)\n",
    "\n",
    "# Create a DataFrame for the scaled data\n",
    "df_zscore = DataFrame(scaled_features, columns=data.columns, index=data.index)\n",
    "\n",
    "# Insert the target variable at the 6th index\n",
    "df_zscore.insert(6, target, target_data)\n",
    "\n",
    "# Save the final DataFrame\n",
    "df_zscore.to_csv(f\"{path_to_directory}/datasets/{file_tag}_scaled_zscore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Methodollogy Standard Scaler</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from pandas import DataFrame, read_csv\n",
    "from matplotlib.pyplot import savefig, show, figure\n",
    "from dslabs_functions import plot_multibar_chart, CLASS_EVAL_METRICS, run_KNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from dslabs_functions import DELTA_IMPROVE\n",
    "\n",
    "def evaluate_approach2(\n",
    "    train: DataFrame, test: DataFrame, target: str = \"class\", metric: str = \"accuracy\"\n",
    ") -> dict[str, list]:\n",
    "    # Separate features and target variables\n",
    "    trnY = train.pop(target).values\n",
    "    trnX: ndarray = train.values\n",
    "    tstY = test.pop(target).values\n",
    "    tstX: ndarray = test.values\n",
    "\n",
    "    # Evaluate KNN\n",
    "    eval_KNN: dict[str, float] = run_KNN(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    \n",
    "    # Create evaluation dictionary\n",
    "    eval: dict[str, list] = {}\n",
    "    for met in CLASS_EVAL_METRICS:\n",
    "        eval[met] = [eval_KNN[met]]  # Only store KNN results as a single-entry list\n",
    "\n",
    "    return eval\n",
    "\n",
    "new_data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_scaled_zscore.csv\")\n",
    "new_data = new_data.head(200000)\n",
    "\n",
    "train, test = train_test_split(new_data, test_size=0.2, random_state=42)\n",
    "\n",
    "figure()\n",
    "eval_standart: dict[str, list] = evaluate_approach2(train, test, target=target, metric=\"recall\")\n",
    "print(eval_standart)\n",
    "plot_multibar_chart(\n",
    "    [\"KNN\"], eval_standart, title=\"Standard scaler evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - z-score: KNN results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MinMax Scaler</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import DataFrame, Series, read_csv\n",
    "\n",
    "# Load data\n",
    "data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_drop_outliers.csv\")\n",
    "\n",
    "target = \"JURISDICTION_CODE\"\n",
    "vars: list[str] = data.columns.to_list()\n",
    "\n",
    "# Separate target column\n",
    "target_data: Series = data.pop(target).copy()\n",
    "\n",
    "# Apply MinMaxScaler to the remaining columns\n",
    "transf: MinMaxScaler = MinMaxScaler(feature_range=(0, 1), copy=True).fit(data)\n",
    "df_minmax = DataFrame(transf.transform(data), index=data.index, columns=data.columns)\n",
    "\n",
    "# Reinsert the target column at the 6th index\n",
    "df_minmax.insert(6, target, target_data)\n",
    "\n",
    "# Rename columns to match original variable names\n",
    "df_minmax.columns = vars\n",
    "\n",
    "# Save the resulting DataFrame\n",
    "df_minmax.to_csv(f\"{path_to_directory}/datasets/{file_tag}_scaled_minmax.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Methodollogy MinMax Scaler</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data: DataFrame = read_csv(f\"{path_to_directory}/datasets/{file_tag}_scaled_minmax.csv\", na_values=\"\", index_col=None, sep=',', decimal='.')\n",
    "new_data = new_data.head(200000)\n",
    "\n",
    "train, test = train_test_split(new_data, test_size=0.2, random_state=42)\n",
    "\n",
    "figure()\n",
    "eval_minmax: dict[str, list] = evaluate_approach2(train, test, target=target, metric=\"recall\")\n",
    "print(eval_minmax)\n",
    "plot_multibar_chart(\n",
    "    [\"KNN\"], eval_minmax, title=\"MinMax evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - min-max: KNN results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformed data in boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import subplots, show\n",
    "\n",
    "fig, axs = subplots(1, 3, figsize=(20, 10), squeeze=False)\n",
    "axs[0, 1].set_title(\"Original data\")\n",
    "data.boxplot(ax=axs[0, 0])\n",
    "axs[0, 0].set_title(\"Z-score normalization\")\n",
    "df_zscore.boxplot(ax=axs[0, 1])\n",
    "axs[0, 2].set_title(\"MinMax normalization\")\n",
    "df_minmax.boxplot(ax=axs[0, 2])\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling method evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = {\"missing\": eval_missing, \"standard_scaler\": eval_standart, \"minmax\": eval_minmax}\n",
    "\n",
    "aux_best_eval = best_eval\n",
    "best_mean = -float(\"inf\")\n",
    "mean = -float(\"inf\")\n",
    "best_eval = None\n",
    "\n",
    "for eval_name, eval_dict in evaluations.items():\n",
    "    if eval_name == \"missing\":\n",
    "        mean = sum(eval_dict[metric][1] for metric in [\"accuracy\", \"recall\", \"precision\", \"auc\", \"f1\"]) / 5\n",
    "    else:\n",
    "        mean = sum(eval_dict[metric][0] for metric in [\"accuracy\", \"recall\", \"precision\", \"auc\", \"f1\"]) / 5\n",
    "    \n",
    "    # Print means\n",
    "    print(f\"{eval_name}: Mean = {mean}\")\n",
    "\n",
    "    # Update the best mean\n",
    "    if mean > best_mean:\n",
    "        best_mean = mean\n",
    "        best_eval = eval_name\n",
    "\n",
    "# Print the best mean scores\n",
    "print(f\"\\nContinuing with {best_eval} with mean {best_mean}\")\n",
    "\n",
    "# Determine the filename based on the best evaluations\n",
    "if best_eval == f\"{aux_best_eval}\":\n",
    "    filename = f\"{path_to_directory}/datasets/{file_tag}_missing_values.csv\"\n",
    "    best_eval_dict = eval_missing\n",
    "elif best_eval == \"standard_scaler\":\n",
    "    filename = f\"{path_to_directory}/datasets/{file_tag}_scaled_zscore.csv\"\n",
    "    best_eval_dict = eval_standart\n",
    "elif best_eval == \"minmax\":\n",
    "    filename = f\"{path_to_directory}/datasets/{file_tag}_scaled_minmax.csv\"\n",
    "    best_eval_dict = eval_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, scaling didn't improve the results, so we won't use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Engineering**\n",
    "## **Feature Selection**\n",
    "### Dropping Low Variance Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, Index, read_csv\n",
    "from dslabs_functions import (\n",
    "    select_low_variance_variables,\n",
    "    study_variance_for_feature_selection,\n",
    "    apply_feature_selection,\n",
    "    select_redundant_variables,\n",
    "    study_redundancy_for_feature_selection,\n",
    ")\n",
    "\n",
    "\n",
    "def select_low_variance_variables(\n",
    "    data: DataFrame, max_threshold: float, target: str = \"class\"\n",
    ") -> list:\n",
    "    summary5: DataFrame = data.describe()\n",
    "    vars2drop: Index[str] = summary5.columns[\n",
    "        summary5.loc[\"std\"] * summary5.loc[\"std\"] < max_threshold\n",
    "    ]\n",
    "    vars2drop = vars2drop.drop(target) if target in vars2drop else vars2drop\n",
    "    return list(vars2drop.values)\n",
    "\n",
    "\n",
    "target = \"JURISDICTION_CODE\"\n",
    "train: DataFrame = read_csv(filepath_or_buffer=filename)\n",
    "\n",
    "print(\"Original variables\", train.columns.to_list())\n",
    "vars2drop: list[str] = select_low_variance_variables(train, 3, target=target)\n",
    "print(\"Variables to drop\", vars2drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from matplotlib.pyplot import savefig, show, figure\n",
    "from dslabs_functions import HEIGHT, plot_multiline_chart\n",
    "from pandas import Index\n",
    "\n",
    "\n",
    "def evaluate_approach(\n",
    "    train: DataFrame, test: DataFrame, target: str = \"class\", metric: str = \"accuracy\"\n",
    ") -> dict[str, list]:\n",
    "    trnY = train.pop(target).values\n",
    "    trnX: ndarray = train.values\n",
    "    tstY = test.pop(target).values\n",
    "    tstX: ndarray = test.values\n",
    "    eval: dict[str, list] = {}\n",
    "\n",
    "    eval_NB: dict[str, float] = run_NB(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    eval_KNN: dict[str, float] = run_KNN(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    if eval_NB != {} and eval_KNN != {}:\n",
    "        for met in CLASS_EVAL_METRICS:\n",
    "            eval[met] = [eval_NB[met], eval_KNN[met]]\n",
    "    return eval\n",
    "\n",
    "def study_variance_for_feature_selection(\n",
    "    train: DataFrame,\n",
    "    test: DataFrame,\n",
    "    target: str = \"class\",\n",
    "    max_threshold: float = 1,\n",
    "    lag: float = 0.05,\n",
    "    metric: str = \"accuracy\",\n",
    "    file_tag: str = \"\",\n",
    ") -> dict:\n",
    "    options: list[float] = [\n",
    "        round(i * lag, 3) for i in range(1, ceil(max_threshold / lag + lag))\n",
    "    ]\n",
    "    results: dict[str, list] = {\"NB\": [], \"KNN\": []}\n",
    "    summary5: DataFrame = train.describe()\n",
    "    for thresh in options:\n",
    "        vars2drop: Index[str] = summary5.columns[\n",
    "            summary5.loc[\"std\"] * summary5.loc[\"std\"] < thresh\n",
    "        ]\n",
    "        vars2drop = vars2drop.drop(target) if target in vars2drop else vars2drop\n",
    "\n",
    "        train_copy: DataFrame = train.drop(vars2drop, axis=1, inplace=False)\n",
    "        test_copy: DataFrame = test.drop(vars2drop, axis=1, inplace=False)\n",
    "        eval: dict[str, list] | None = evaluate_approach(\n",
    "            train_copy, test_copy, target=target, metric=metric\n",
    "        )\n",
    "        if eval is not None:\n",
    "            results[\"NB\"].append(eval[metric][0])\n",
    "            results[\"KNN\"].append(eval[metric][1])\n",
    "\n",
    "    plot_multiline_chart(\n",
    "        options,\n",
    "        results,\n",
    "        title=f\"Drop low variance variables study\",\n",
    "        xlabel=\"variance threshold\",\n",
    "        ylabel=metric,\n",
    "        percentage=True,\n",
    "    )\n",
    "    savefig(f\"{path_to_directory}/images/Set 1 - Drop low variance variables study.png\", bbox_inches='tight')\n",
    "    return results\n",
    "\n",
    "target = \"JURISDICTION_CODE\"\n",
    "train: DataFrame = read_csv(filepath_or_buffer=filename)\n",
    "\n",
    "eval_metric = \"recall\"\n",
    "train = train.head(200000)\n",
    "train, test = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "figure()\n",
    "study_variance_for_feature_selection(\n",
    "    train,\n",
    "    test,\n",
    "    target=target,\n",
    "    max_threshold=1.1,\n",
    "    lag=0.1,\n",
    "    metric=eval_metric,\n",
    "    file_tag=file_tag,\n",
    ")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_selection(\n",
    "    train: DataFrame,\n",
    "    test: DataFrame,\n",
    "    vars2drop: list,\n",
    "    filename: str = \"\",\n",
    "    tag: str = \"\",\n",
    ") -> tuple[DataFrame, DataFrame]:\n",
    "    train_copy: DataFrame = train.drop(vars2drop, axis=1, inplace=False)\n",
    "    train_copy.to_csv(f\"{filename}_train_{tag}.csv\", index=False)\n",
    "    test_copy: DataFrame = test.drop(vars2drop, axis=1, inplace=False)\n",
    "    test_copy.to_csv(f\"{filename}_test_{tag}.csv\", index=False)\n",
    "    return train_copy, test_copy\n",
    "\n",
    "\n",
    "vars2drop: list[str] = select_low_variance_variables(\n",
    "    train, max_threshold=0.9, target=target\n",
    ")\n",
    "train_cp, test_cp = apply_feature_selection(\n",
    "    train, test, vars2drop, filename=f\"{path_to_directory}/datasets/{file_tag}\", tag=\"lowvar\"\n",
    ")\n",
    "print(f\"Original data: train={train.shape}, test={test.shape}\")\n",
    "print(f\"After low variance FS: train_cp={train_cp.shape}, test_cp={test_cp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "eval_lowvar: dict[str, list] = evaluate_approach(train_cp, test_cp, target=target, metric=\"recall\")\n",
    "print(eval_lowvar)\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval_lowvar, title=\"Drop low variance variables evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - Drop low variance variables results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Redundant Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "\n",
    "def select_redundant_variables(\n",
    "    data: DataFrame, min_threshold: float = 0.90, target: str = \"class\"\n",
    ") -> list:\n",
    "    df: DataFrame = data.drop(target, axis=1, inplace=False)\n",
    "    corr_matrix: DataFrame = abs(df.corr())\n",
    "    variables: Index[str] = corr_matrix.columns\n",
    "    vars2drop: list = []\n",
    "    for v1 in variables:\n",
    "        if v1 not in vars2drop:\n",
    "            vars_corr: Series = (corr_matrix[v1]).loc[corr_matrix[v1] >= min_threshold]\n",
    "            vars_corr.drop(v1, inplace=True)\n",
    "            if len(vars_corr) > 0:\n",
    "                lst_corr = list(vars_corr.index)\n",
    "                for v2 in lst_corr:\n",
    "                    if v2 not in vars2drop:\n",
    "                        vars2drop.append(v2)\n",
    "    return vars2drop\n",
    "\n",
    "print(\"Original variables\", train.columns.values)\n",
    "vars2drop: list[str] = select_redundant_variables(\n",
    "    train, target=target, min_threshold=0.5\n",
    ")\n",
    "print(\"Variables to drop\", vars2drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_redundancy_for_feature_selection(\n",
    "    train: DataFrame,\n",
    "    test: DataFrame,\n",
    "    target: str = \"class\",\n",
    "    min_threshold: float = 0.90,\n",
    "    lag: float = 0.05,\n",
    "    metric: str = \"accuracy\",\n",
    "    file_tag: str = \"\",\n",
    ") -> dict:\n",
    "    options: list[float] = [\n",
    "        round(min_threshold + i * lag, 3)\n",
    "        for i in range(ceil((1 - min_threshold) / lag) + 1)\n",
    "    ]\n",
    "\n",
    "    # Create a list to track valid thresholds\n",
    "    valid_thresholds = []\n",
    "\n",
    "    df: DataFrame = train.drop(target, axis=1, inplace=False)\n",
    "    corr_matrix: DataFrame = abs(df.corr())\n",
    "    variables: Index[str] = corr_matrix.columns\n",
    "    results: dict[str, list] = {\"NB\": [], \"KNN\": []}\n",
    "    for thresh in options:\n",
    "        vars2drop: list = []\n",
    "        for v1 in variables:\n",
    "            if v1 not in vars2drop:\n",
    "                vars_corr: Series = (corr_matrix[v1]).loc[corr_matrix[v1] >= thresh]\n",
    "                vars_corr.drop(v1, inplace=True)\n",
    "                if len(vars_corr) > 0:\n",
    "                    lst_corr = list(vars_corr.index)\n",
    "                    for v2 in lst_corr:\n",
    "                        if v2 not in vars2drop:\n",
    "                            vars2drop.append(v2)\n",
    "\n",
    "        train_copy: DataFrame = train.drop(vars2drop, axis=1, inplace=False)\n",
    "        test_copy: DataFrame = test.drop(vars2drop, axis=1, inplace=False)\n",
    "        eval: dict | None = evaluate_approach(\n",
    "            train_copy, test_copy, target=target, metric=metric\n",
    "        )\n",
    "        if eval:  # Check if eval is not empty\n",
    "            valid_thresholds.append(thresh)\n",
    "            results[\"NB\"].append(eval[metric][0])\n",
    "            results[\"KNN\"].append(eval[metric][1])\n",
    "        else:\n",
    "            print(f\"Skipping threshold {thresh} due to empty evaluation results.\")\n",
    "\n",
    "    plot_multiline_chart(\n",
    "        # options,\n",
    "        valid_thresholds,\n",
    "        results,\n",
    "        title=f\"Drop redundant variables study\",\n",
    "        xlabel=\"correlation threshold\",\n",
    "        ylabel=metric,\n",
    "        percentage=True,\n",
    "    )\n",
    "    savefig(f\"{path_to_directory}/images/Set 1 - Drop redundant variables study.png\", bbox_inches='tight')\n",
    "    return results\n",
    "\n",
    "\n",
    "target = \"JURISDICTION_CODE\"\n",
    "train: DataFrame = read_csv(filepath_or_buffer=filename)\n",
    "train = train.head(100000)\n",
    "\n",
    "eval_metric = \"recall\"\n",
    "train, test = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "# figure(figsize=(2 * HEIGHT, HEIGHT))\n",
    "figure()\n",
    "study_redundancy_for_feature_selection(\n",
    "    train,\n",
    "    test,\n",
    "    target=target,\n",
    "    min_threshold=0.25,\n",
    "    lag=0.05,\n",
    "    metric=eval_metric,\n",
    "    file_tag=file_tag,\n",
    ")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars2drop: list[str] = select_redundant_variables(\n",
    "    train, min_threshold=0.8, target=target\n",
    ")\n",
    "train_cp, test_cp = apply_feature_selection(\n",
    "    train, test, vars2drop, filename=f\"{path_to_directory}/datasets/{file_tag}\", tag=\"redundant\"\n",
    ")\n",
    "print(f\"Original data: train={train.shape}, test={test.shape}\")\n",
    "print(f\"After redundant FS: train_cp={train_cp.shape}, test_cp={test_cp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "eval_redundant: dict[str, list] = evaluate_approach(train_cp, test_cp, target=target, metric=\"recall\")\n",
    "print(eval_redundant)\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval_redundant, title=\"Drop redundant variables evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - Drop redundant variables results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering method evaluation\n",
    "Assuming the best_eval comes from missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = {\"original\": eval_missing, \"lowvar\": eval_lowvar, \"redundant\": eval_redundant}\n",
    "\n",
    "aux_best_eval = best_eval\n",
    "best_mean = -float(\"inf\")\n",
    "best_mean_NB = -float(\"inf\")\n",
    "best_mean_KNN = -float(\"inf\")\n",
    "best_eval = None\n",
    "best_eval_NB = None\n",
    "best_eval_KNN = None\n",
    "\n",
    "for eval_name, eval_dict in evaluations.items():\n",
    "    # Calculate mean for NB and KNN using generator expressions\n",
    "    mean_NB = sum(eval_dict[metric][0] for metric in [\"accuracy\", \"recall\", \"precision\", \"auc\", \"f1\"]) / 5\n",
    "    mean_KNN = sum(eval_dict[metric][1] for metric in [\"accuracy\", \"recall\", \"precision\", \"auc\", \"f1\"]) / 5\n",
    "    \n",
    "    # Print means\n",
    "    print(f\"{eval_name}: Mean NB = {mean_NB}, Mean KNN = {mean_KNN}\")\n",
    "\n",
    "    # Update the best mean for NB\n",
    "    if mean_NB > best_mean_NB:\n",
    "        best_mean_NB = mean_NB\n",
    "        best_eval_NB = eval_name\n",
    "\n",
    "    # Update the best mean for KNN\n",
    "    if mean_KNN > best_mean_KNN:\n",
    "        best_mean_KNN = mean_KNN\n",
    "        best_eval_KNN = eval_name\n",
    "\n",
    "# Print the best mean scores\n",
    "print(f\"\\nBest Mean NB: {best_mean_NB} from {best_eval_NB}\")\n",
    "print(f\"Best Mean KNN: {best_mean_KNN} from {best_eval_KNN}\")\n",
    "\n",
    "if best_mean_NB > best_mean_KNN:\n",
    "    best_mean = best_mean_NB\n",
    "    best_eval = best_eval_NB\n",
    "else:\n",
    "    best_mean = best_mean_KNN\n",
    "    best_eval = best_eval_KNN\n",
    "\n",
    "# Print the best mean scores\n",
    "print(f\"\\nContinuing with {best_eval} with mean {best_mean}\")\n",
    "\n",
    "# Determine the filename based on the best evaluations\n",
    "if best_eval == f\"{aux_best_eval}\":\n",
    "    filename = f\"{path_to_directory}/datasets/{file_tag}_missing_values.csv\"\n",
    "    best_eval_dict = best_eval_dict\n",
    "elif best_eval == \"lowvar\":\n",
    "    filename_train = f\"{path_to_directory}/datasets/{file_tag}_train_lowvar.csv\"\n",
    "    filename_test = f\"{path_to_directory}/datasets/{file_tag}_test_lowvar.csv\"\n",
    "    best_eval_dict = eval_lowvar\n",
    "elif best_eval == \"redundant\":\n",
    "    filename_train = f\"{path_to_directory}/datasets/{file_tag}_train_redundant.csv\"\n",
    "    filename_test = f\"{path_to_directory}/datasets/{file_tag}_test_redundant.csv\"\n",
    "    best_eval_dict = eval_redundant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Droping redundant values gave better results, so we will proceed with that daatset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Balancing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, concat, DataFrame, Series\n",
    "from matplotlib.pyplot import figure, show\n",
    "from dslabs_functions import plot_bar_chart\n",
    "\n",
    "original: DataFrame = read_csv(filepath_or_buffer=f\"{filename_train}\", sep=\",\", decimal=\".\")\n",
    "test: DataFrame = read_csv(filepath_or_buffer=f\"{filename_test}\", sep=\",\", decimal=\".\")\n",
    "\n",
    "target_count: Series = original[target].value_counts()\n",
    "positive_class = target_count.idxmin()\n",
    "negative_class = target_count.idxmax()\n",
    "\n",
    "print(\"Minority class=\", positive_class, \":\", target_count[positive_class])\n",
    "print(\"Majority class=\", negative_class, \":\", target_count[negative_class])\n",
    "print(\n",
    "    \"Proportion:\",\n",
    "    round(target_count[positive_class] / target_count[negative_class], 2),\n",
    "    \": 1\",\n",
    ")\n",
    "values: dict[str, list] = {\n",
    "    \"Original\": [target_count[positive_class], target_count[negative_class]]\n",
    "}\n",
    "\n",
    "figure()\n",
    "plot_bar_chart(\n",
    "    target_count.index.to_list(), target_count.to_list(), title=\"Class balance\"\n",
    ")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positives: Series = original[original[target] == positive_class]\n",
    "df_negatives: Series = original[original[target] == negative_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Undersampling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg_sample: DataFrame = DataFrame(df_negatives.sample(len(df_positives), random_state=42))\n",
    "df_under: DataFrame = concat([df_positives, df_neg_sample], axis=0)\n",
    "df_under.to_csv(f\"{path_to_directory}/datasets/{file_tag}_undersampling.csv\", index=False)\n",
    "\n",
    "print(\"Minority class=\", positive_class, \":\", len(df_positives))\n",
    "print(\"Majority class=\", negative_class, \":\", len(df_neg_sample))\n",
    "print(\"Proportion:\", round(len(df_positives) / len(df_neg_sample), 2), \": 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "eval_undersampling: dict[str, list] = evaluate_approach(df_under, test, target=target, metric=\"recall\")\n",
    "print(eval_undersampling)\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval_undersampling, title=\"Undersampling evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - Undersampling results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Oversampling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_sample: DataFrame = DataFrame(\n",
    "    df_positives.sample(len(df_negatives), replace=True, random_state=42)\n",
    ")\n",
    "df_over: DataFrame = concat([df_pos_sample, df_negatives], axis=0)\n",
    "df_over.to_csv(f\"{path_to_directory}/datasets/{file_tag}_oversampling.csv\", index=False)\n",
    "\n",
    "print(\"Minority class=\", positive_class, \":\", len(df_pos_sample))\n",
    "print(\"Majority class=\", negative_class, \":\", len(df_negatives))\n",
    "print(\"Proportion:\", round(len(df_pos_sample) / len(df_negatives), 2), \": 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because CLASS column was removed\n",
    "test: DataFrame = read_csv(filepath_or_buffer=f\"{filename_test}\", sep=\",\", decimal=\".\")\n",
    "\n",
    "figure()\n",
    "eval_oversampling: dict[str, list] = evaluate_approach(df_over, test, target=target, metric=\"recall\")\n",
    "print(eval_oversampling)\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval_oversampling, title=\"Oversampling evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - Oversampling results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SMOTE</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from pandas import Series\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "smote: SMOTE = SMOTE(sampling_strategy=\"minority\", random_state=RANDOM_STATE)\n",
    "y = original.pop(target).values\n",
    "X: ndarray = original.values\n",
    "smote_X, smote_y = smote.fit_resample(X, y)\n",
    "df_smote: DataFrame = concat([DataFrame(smote_X), DataFrame(smote_y)], axis=1)\n",
    "df_smote.columns = list(original.columns) + [target]\n",
    "df_smote.to_csv(f\"{path_to_directory}/datasets/{file_tag}_smote.csv\", index=False)\n",
    "\n",
    "smote_target_count: Series = Series(smote_y).value_counts()\n",
    "print(\"Minority class=\", positive_class, \":\", smote_target_count[positive_class])\n",
    "print(\"Majority class=\", negative_class, \":\", smote_target_count[negative_class])\n",
    "print(\n",
    "    \"Proportion:\",\n",
    "    round(smote_target_count[positive_class] / smote_target_count[negative_class], 2),\n",
    "    \": 1\",\n",
    ")\n",
    "print(df_smote.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because CLASS column was removed\n",
    "test: DataFrame = read_csv(filepath_or_buffer=f\"{filename_test}\", sep=\",\", decimal=\".\")\n",
    "\n",
    "figure()\n",
    "eval_smote: dict[str, list] = evaluate_approach(df_smote, test, target=target, metric=\"recall\")\n",
    "print(eval_smote)\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval_smote, title=\"SMOTE evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"{path_to_directory}/images/Set 1 - SMOTE results.png\", bbox_inches='tight')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing method evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Continuing from {best_eval} with mean {best_mean}\\n\")\n",
    "\n",
    "evaluations = {f\"{best_eval}\": best_eval_dict, \"undersampling\": eval_undersampling, \"oversampling\": eval_oversampling, \"smote\": eval_smote}\n",
    "# evaluations = {f\"{best_eval}\": best_eval_dict, \"oversampling\": eval_oversampling, \"smote\": eval_smote}\n",
    "\n",
    "aux_best_eval = best_eval\n",
    "best_mean = -float(\"inf\")\n",
    "best_mean_NB = -float(\"inf\")\n",
    "best_mean_KNN = -float(\"inf\")\n",
    "best_eval = None\n",
    "best_eval_NB = None\n",
    "best_eval_KNN = None\n",
    "\n",
    "for eval_name, eval_dict in evaluations.items():\n",
    "    # Calculate mean for NB and KNN using generator expressions\n",
    "    mean_NB = sum(eval_dict[metric][0] for metric in [\"accuracy\", \"recall\", \"precision\", \"auc\", \"f1\"]) / 5\n",
    "    mean_KNN = sum(eval_dict[metric][1] for metric in [\"accuracy\", \"recall\", \"precision\", \"auc\", \"f1\"]) / 5\n",
    "    \n",
    "    # Print means\n",
    "    print(f\"{eval_name}: Mean NB = {mean_NB}, Mean KNN = {mean_KNN}\")\n",
    "\n",
    "    # Update the best mean for NB\n",
    "    if mean_NB > best_mean_NB:\n",
    "        best_mean_NB = mean_NB\n",
    "        best_eval_NB = eval_name\n",
    "\n",
    "    # Update the best mean for KNN\n",
    "    if mean_KNN > best_mean_KNN:\n",
    "        best_mean_KNN = mean_KNN\n",
    "        best_eval_KNN = eval_name\n",
    "\n",
    "# Print the best mean scores\n",
    "print(f\"\\nBest Mean NB: {best_mean_NB} from {best_eval_NB}\")\n",
    "print(f\"Best Mean KNN: {best_mean_KNN} from {best_eval_KNN}\")\n",
    "\n",
    "if best_mean_NB > best_mean_KNN:\n",
    "    best_mean = best_mean_NB\n",
    "    best_eval = best_eval_NB\n",
    "else:\n",
    "    best_mean = best_mean_KNN\n",
    "    best_eval = best_eval_KNN\n",
    "\n",
    "# Print the best mean scores\n",
    "print(f\"\\nContinuing with {best_eval} with mean {best_mean}\")\n",
    "\n",
    "# Determine the filename based on the best evaluations\n",
    "if best_eval == f\"{aux_best_eval}\":\n",
    "    filename_train = f\"{path_to_directory}/datasets/{file_tag}_train_lowvar.csv\"\n",
    "    best_eval_dict = eval_lowvar\n",
    "elif best_eval == \"undersampling\":\n",
    "    filename_train = f\"{path_to_directory}/datasets/{file_tag}_undersampling.csv\"\n",
    "elif best_eval == \"oversampling\":\n",
    "    filename_train = f\"{path_to_directory}/datasets/{file_tag}_oversampling.csv\"\n",
    "elif best_eval == \"smote\":\n",
    "    filename_train = f\"{path_to_directory}/datasets/{file_tag}_smote.csv\"\n",
    "\n",
    "filename_test = filename_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Best model seems to be with Encoding, Drop Missing Values, no Outliers inputation, no Scaling, droping Reduntant variables and no balancing</h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
